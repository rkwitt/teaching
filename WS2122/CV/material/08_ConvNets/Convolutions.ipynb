{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "\n",
    "Roland Kwitt, 2020\n",
    "\n",
    "## <font color='blue'>Convolutions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "- [Motivation](#Motivation)\n",
    "- [Conventions & Terminology](#Conventions-and-Terminology)\n",
    "- [1D convolution](#Convolution-in-1D)\n",
    "- [2D convolution](#Convolution-in-2D)\n",
    "- [Convolution in neural networks](#Convolution-in-neural-networks)\n",
    "- [Grouped convolution](#Grouped-convolution)\n",
    "- [Resources](#Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Lets say we have input signals of large dimension, but with a very distinct inherent structure, e.g., images or accoustic signals.\n",
    "\n",
    "If we would handle these signals simply as ''vectors'', we would immediately run into problems. Just think of a relatively small $256 \\times 256$ RGB image. Mapping such an input, when considered as a vector, with a linear layer to, say, $\\mathbb{R}^{1000}$, would require a matrix of size\n",
    "\n",
    "$$1000 \\times (256\\cdot 256 \\cdot 3) = 100 \\times 196608$$\n",
    "\n",
    "This already requires **750 MB** of memory, assuming 32-bit floating point numbers (and this would just be the memory footprint of the first layer!)\n",
    "\n",
    "Possibly even more concerning is the fact that processing such signals (e.g., images) should have some degree of invariance (e.g., wrt. translation). Put differently, a *representation meaningful at\n",
    "a certain location can / should be used everywhere*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conventions and Terminology\n",
    "\n",
    "As we know, everything in PyTorch is handeled in *batches* of size `N`. A batch of size `N=1` would mean only a single input. In the context of deep learning with PyTorch, the batch size is the first dimension of a tensor.    \n",
    "\n",
    "For convolution operations, we call the second dimension the **number of channels** and any following dimensions the **channel dimensions**.\n",
    "\n",
    "A tensor of size \n",
    "`1 x 1 x 10` would mean\n",
    "\n",
    "- batch size: `1`\n",
    "- \\#channels: `1`\n",
    "- channel size: `10`\n",
    "\n",
    "A tensor of size `1 x 10 x 1` would mean\n",
    "\n",
    "- batch size: `1`\n",
    "- \\#channels: `10`\n",
    "- channel size: `1`\n",
    "\n",
    "A tensor of size `100 x 3 x 32 x 32` would mean\n",
    "\n",
    "- batch size: `100`\n",
    "- \\#channels: `3`\n",
    "- channel size: `32 x 32`\n",
    "\n",
    "A typical example for such a tensor would be 100 RGB images of width and height 32 pixel.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "We will see that convolution layers in neural networks will take input tensors of this form and output tensors of this form.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Convolution in 1D\n",
    "\n",
    "Say we have a signal $\\mathbf{x}$, written here as a row vector with $W$ elements, i.e.,\n",
    "\n",
    "$$\\mathbf{x} = [x_1,\\ldots,x_W]$$\n",
    "\n",
    "and a **convolution kernel** $\\mathbf{u}$ with $w$ elements, i.e.,\n",
    "\n",
    "$$\\mathbf{u} = [u_1,\\ldots,u_w]$$\n",
    "\n",
    "Then, *convolving* $\\mathbf{x}$ with $\\mathbf{u}$ means\n",
    "\n",
    "$$[\\mathbf{x} * \\mathbf{u}]_i = \\sum_{j=1}^w x_{i-1+j} u_j$$\n",
    "\n",
    "where $[\\mathbf{x} * \\mathbf{u}]_i$ denotes the output of the convolution operation at the $i$-th position.\n",
    "\n",
    "**Example**\n",
    "\n",
    "$$\\mathbf{x} = [1,2,3,4,5,6,7,8,9,10]$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\mathbf{u} = [1,2,3]$$\n",
    "\n",
    "$$[\\mathbf{x} * \\mathbf{u}]_1 = \\sum_{j=1}^3 x_{i-1+j} u_j = x_1u_1 + x_2u_2 + x_3u_3 = 14$$\n",
    "\n",
    "where we started indexing by $1$. This is different from convolution in signal processing since we both visit signal and kernel elements in *increasing* index order.\n",
    "\n",
    "**Illustration**\n",
    "\n",
    "<img src=\"1Dconv.svg\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "The parameters of this convolution operation are the <font color='blue'>values of the convolution kernel</font>. In the previous example, we thus have 3 parameters (as we did not include bias). If we do include bias, the number of paramters would be 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]]]\n",
      "[[[14. 20. 26. 32. 38. 44. 50. 56.]]]\n",
      "Check (pos 0): (1*2 + 2*2 + 3*3) = 14\n",
      "Check (pos 1): (1*2 + 2*3 + 3*4) = 20\n"
     ]
    }
   ],
   "source": [
    "# create toy input [1,2,3,4,...,10] \n",
    "x = torch.tensor(list(np.arange(1, 11)), dtype=torch.float32)\n",
    "\n",
    "# view the input as a 1x1x10 tensor, i.e., batch-size 1, 1x10 inputs\n",
    "x = x.view(1, 1, 10)\n",
    "\n",
    "# 1D convolution\n",
    "m = nn.Conv1d(in_channels=1,   # one input channel\n",
    "              out_channels=1,  # one output channel\n",
    "              kernel_size=3,   # use a kernel size of 3\n",
    "              stride=1,        # move the kernel along by steps of 1\n",
    "              padding=0,       # do not pad the input\n",
    "              bias=False)      # do not include bias (i.e., the b in Ax+b)\n",
    "\n",
    "# directly set the convolution kernel paramters (for demonstration)\n",
    "m.weight.data = torch.tensor([1, 2, 3], dtype=torch.float32).view(1, 1, 3)\n",
    "\n",
    "# forward through the 1D conv. operation\n",
    "y = m(x)\n",
    "\n",
    "print(x.numpy())\n",
    "print(y.detach().numpy())\n",
    "\n",
    "# Note the the formula from above also works when we \n",
    "# start indexing by 0\n",
    "print('Check (pos 0): (1*2 + 2*2 + 3*3) = 14')\n",
    "print('Check (pos 1): (1*2 + 2*3 + 3*4) = 20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that a linear layer could implement the *same* operation here, but we would need to be careful when we compute gradient updates, as we would have to make sure that the *zero* entries stay zero (can be done via masking for instance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding**\n",
    "\n",
    "Per default, *padding* is set to zero, so the output (in the previous example) is of size `1 x 1 x 8`; in other words, a kernel of size 3 with a stride of 1 can be applied 8 times.\n",
    "Next, lets pad with *one zero entry* on both sides and still use a stride of 1; in this setting, we preserve the input dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(list(np.arange(1,11)), dtype=torch.float32).view(1,1,10)\n",
    "m = nn.Conv1d(in_channels=1, \n",
    "              out_channels=1, \n",
    "              kernel_size=3, \n",
    "              stride=1, \n",
    "              padding=1, \n",
    "              bias=False) \n",
    "y = m(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple convolution kernels**\n",
    "\n",
    "We do not have to use just one kernel, we can use as many as we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(list(np.arange(1, 11)), dtype=torch.float32).view(1, 1, 10)\n",
    "\n",
    "m = nn.Conv1d(in_channels=1, \n",
    "              out_channels=5, \n",
    "              kernel_size=3, \n",
    "              stride=1, \n",
    "              padding=1, \n",
    "              bias=False)\n",
    "y = m(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we not only convolve with one kernel but with **5 (different) kernels**. Hence, we obtain five output channels of size 10. The parameter tensor of the 1D convolution layer from the last example is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(m.weight.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, 5 kernels of size $1 \\times 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stride**\n",
    "\n",
    "Lets not shift the kernel by one position a time, but by two. This effectively *downsamples* the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4])\n",
      "#Parameters:  3\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(list(np.arange(1, 11)), dtype=torch.float32).view(1, 1, 10)\n",
    "\n",
    "m = nn.Conv1d(in_channels=1, \n",
    "              out_channels=1,\n",
    "              kernel_size=3, \n",
    "              stride=2, \n",
    "              padding=0, \n",
    "              bias=False)\n",
    "y = m(x)\n",
    "print(y.size())\n",
    "\n",
    "n_params = 0\n",
    "for p in m.parameters(): \n",
    "    n_params += p.numel()\n",
    "print('#Parameters: ', n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Bias.</b> In all previous examples, we disabled the bias term. Per default, this is enabled. If so, we have one additional parameter. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4])\n",
      "#Parameters:  4\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(list(np.arange(1, 11)), dtype=torch.float32).view(1, 1, 10)\n",
    "m = nn.Conv1d(1, 1, 3, stride=2, padding=0, bias=True)\n",
    "y = m(x)\n",
    "print(y.size())\n",
    "\n",
    "n_params = 0\n",
    "for p in m.parameters(): \n",
    "    n_params += p.numel()\n",
    "print('#Parameters: ', n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Also note that at all positions where we apply the kernel, we use the same kernel paramters. We call this <b>weight sharing</b>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution as a linear operation\n",
    "\n",
    "In fact, it's fairly easy to see that a convolution operation can be implemented as a linear operation.\n",
    "\n",
    "<img src=\"WeightSharing.svg\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass through 1D convolution layer\n",
      "[14. 20. 26.]\n",
      "\n",
      "Forward pass through a corresponding linear layer\n",
      "[14. 20. 26.]\n"
     ]
    }
   ],
   "source": [
    "# input is a 1x5 tensor\n",
    "x_inp = torch.tensor([[1.,2.,3.,4.,5.]])\n",
    "\n",
    "# 1D convolution layer, 1 input channel, 1 output channel, kernel size 3, no bias\n",
    "layer = nn.Conv1d(1,1,3,bias=False)\n",
    "layer.weight.data = torch.tensor([[[1.,2.,3.]]])\n",
    "print('Forward pass through 1D convolution layer')\n",
    "print(layer(x_inp.unsqueeze(0)).detach().squeeze().numpy())\n",
    "print()\n",
    "\n",
    "# lets implement the same mapping using a linear layer using the following \n",
    "# weight matrix:\n",
    "A = torch.tensor([\n",
    "    [1.,2.,3.,0.,0.],\n",
    "    [0.,1.,2.,3.,0.],\n",
    "    [0.,0.,1.,2.,3.]\n",
    "])\n",
    "\n",
    "lin_layer = nn.Linear(5, 3, bias=False)\n",
    "lin_layer.weight.data = A\n",
    "print('Forward pass through a corresponding linear layer')\n",
    "print(lin_layer(x_inp).detach().squeeze().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Convolution in 2D\n",
    "\n",
    "We will not discuss 2D convolution formally, as the specification of the convolution operation becomes tedious; The principle is quite simple and remains the same as in the 1D case. \n",
    "\n",
    "In the following example, we have an input tensor of size `W x H` (width times height) and `C` channels. The convolution kernel has spatial size `w x h` and also `C` channels.\n",
    "\n",
    "Convolution with this kernel (using `stride=1` and *no* padding) gives a tensor of size `1 x (W-w+1) x (H-h+1)`. If we use 2 kernels, we get, as ouput, a tensor of size `2 x (W-w+1) x (H-h+1)`, etc.\n",
    "\n",
    "<img src=\"2Dconv.svg\" style=\"width: 350px;\"/>\n",
    "\n",
    "**Parameters**: The number of parameters in the first case is `w*h*C` (or `w*h*C+C` if we include the bias). We see that **weight sharing** allows us to effectively process fairly large inputs with relatively few parameters.  \n",
    "\n",
    "Also note that in case we use `K` kernels, the parameters of the convolution layers are stored in a `K x C x w x h` tensor. If bias is included we also have an `K` additional parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter weight : #paramteters = 27\n",
      "Parameter weight : #paramteters = 27\n",
      "Parameter bias : #paramteters = 3\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "m_wo_bias = nn.Conv2d(1, K, 3, stride=2, padding=0, bias=False)\n",
    "m_w_bias = nn.Conv2d(1, K, 3, stride=2, padding=0, bias=True)\n",
    "\n",
    "for (name, p) in m_wo_bias.named_parameters():\n",
    "    print('Parameter {} : #paramteters = {}'.format(name, p.numel()))\n",
    "for (name, p) in m_w_bias.named_parameters():\n",
    "    print('Parameter {} : #paramteters = {}'.format(name, p.numel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology**: When processing inputs with convolution layers having $K$ kernels, we produce $K$ outputs (i.e., the `ouput_channels`). We sometimes also say, we produce $K$ **feature maps**. This makes sense, if we consider, e.g., a $5 \\times 5$ kernel as identifying relevant features and applying this kernel to the full input produces a **feature map** (just think about an edge detection filter for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 30, 30])\n",
      "#Parameters:  55\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 6, 32, 32)\n",
    "m = nn.Conv2d(6, 1, 3, stride=1, padding=0)\n",
    "print(m(x).size())\n",
    "\n",
    "n_params = 0\n",
    "for p in m.parameters(): \n",
    "    n_params += p.numel()\n",
    "print('#Parameters: ', n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we note that the input tensor is of size `10 x 6 x 32 x 32`, i.e., \n",
    "\n",
    "- batch size: `10`\n",
    "- #channels: `6`\n",
    "- channel size: `32 x 32`\n",
    "\n",
    "Our kernel will have size `1 x 6 x 3 x 3`, so `6` channels of size `3 x 3`; the number of channels corresponds to the channels of the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution vs. a linear layer** (aka fully-connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec torch.Size([10, 6144])\n",
      "o_vec torch.Size([10, 900])\n"
     ]
    }
   ],
   "source": [
    "x_vec = x.view(10,-1)\n",
    "o_vec = m(x).view(10,-1)\n",
    "\n",
    "print('x_vec', x_vec.size())\n",
    "print('o_vec', o_vec.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To map the input tensor to the same output (as with our convolution), we would need a mapping\n",
    "\n",
    "$$f: \\mathbb{R}^{6144} \\to \\mathbb{R}^{900}$$\n",
    "\n",
    "i.e., a matrix $\\mathbf{W}$ of size `6144 x 900` having `5529600` parameters! (our conv. layer only has 54 parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Convolution in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cl = nn.Conv2d(3, 10, 3, 2, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cl(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "x = torch.randn(1,3,32,32)\n",
    "out = net(x)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grouped convolution\n",
    "\n",
    "In some applications, it can be necessary to process a multi-channel (e.g., `C` channels) input in a more refined way. \n",
    "\n",
    "Conventionally, a convolution kernel for a `C`-channel input would also have `C` channels, i.e., it operates over all channels of the input.\n",
    "\n",
    "**Example 1**\n",
    "\n",
    "*Say we want to handle each channel of the input seperately*. Assume our input is of size `B x 10 x 32 x 32` and we want to have `50` output channels. This means we obviously need `50` kernels in total, however, each of size `(10/10) x 3 x 3`. This means we need ten `groups`.\n",
    "\n",
    "Now, the first `50/10=5` kernels are applied on the first channel, producing `5` output channels, the next `5` kernels are applied to the second channel and so on. As we have `10` input channels, we get a total of `50` output channels.\n",
    "\n",
    "We also see that the number of input channels, as well as the number of output channels needs to be divisible by the number of groups.\n",
    "\n",
    "**Example 2**\n",
    "\n",
    "Lets change the number of groups from our previous example to `2` (instead of `10` as before) and keep everything else the same.\n",
    "\n",
    "We again need `50` kernels, but now of size `(10/2) x 3 x 3`. \n",
    "There will be `25` kernels in **group 1** and `25` kernels in **group 2**. Group 1 processes the first half of the input channels, group 2 the second half. As the outputs are concatenated (along the channel dimension), we gain obtain `50` output channels.\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "In principle, you can interpret grouped convolution as first splitting the input into #groups (along the channels) and then processing each group by its own convolution layer with an appropriate number of filters and an appropriate filter size.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 3, 3])\n",
      "torch.Size([1, 50, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,10,32,32)\n",
    "m = nn.Conv2d(10,50,3,1,groups=10, bias=False)\n",
    "print(m.weight.data.size())\n",
    "print(m(x).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 5, 3, 3])\n",
      "torch.Size([1, 50, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,10,32,32)\n",
    "m = nn.Conv2d(10,50,3,1,groups=2, bias=False)\n",
    "print(m.weight.data.size())\n",
    "print(m(x).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great resource for understanding convolution is the convolution arithmetic tutorial by Dumoulin & Visin, which can be found [here](https://github.com/vdumoulin/conv_arithmetic)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
