{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision (911.908)\n",
    "\n",
    "## <font color='crimson'>Automatic differentiation (aka AutoGrad)</font>\n",
    "\n",
    "**Changelog**:\n",
    "- *Sep. 2020*: initial version (using PyTorch v1.6) \n",
    "- *Sep. 2021*: adaptations to PyTorch v1.9\n",
    "- *Oct. 2022*: adaptations to PyTorch v1.12.1 + minor fixes\n",
    "- *Oct. 2024*: adaptations to PyTorch v2.3\n",
    "- *Oct. 2025*: adaptations to PyTorch v2.6\n",
    "\n",
    "---\n",
    "\n",
    "In this lecture, we take a look at the mechanics of **automatic differentiation** (aka *AutoGrad* / *AutoDiff*) in PyTorch. These techniques are at the very heart of today's frameworks for learning with neural networks.\n",
    "\n",
    "You can also find the official (tutorial-like) documentation [here](https://pytorch.org/docs/stable/notes/autograd.html).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introductory example](#Introductory-example)\n",
    "- [A  slightly more involved example](#A-slightly-more-involved-example)\n",
    "- [The AutoGrad machinery](#The-AutoGrad-machinery)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Running this code requires torchviz. Install via\n",
    "#\n",
    "# pip install torchviz\n",
    "#\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import agtree2dot\n",
    "\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from utils import visualize_DAG\n",
    "from IPython.display import HTML\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Introductory example\n",
    "\n",
    "In this example, we take the following function (ReLU stands for Rectified Linear Unit, and we will see this later in the lecture)\n",
    "\n",
    "$$f(x;w,b) = \\max\\{wx + b, 0\\} = \\text{ReLU}(wx+b)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\text{ReLU}: x \\mapsto \\text{ReLU}(x) = \\max\\{x, 0\\}\\enspace.$$\n",
    "\n",
    "We first look at its representation as a **computation graph** (with example values for $w,b$ and $x$ added):\n",
    "\n",
    "<img src=\"DAG0.svg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compute  \n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial b} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial b}$$\n",
    "\n",
    "First, \n",
    "\n",
    "$$ \\frac{\\partial v}{\\partial b} = 1 $$\n",
    "\n",
    "and (using a [sub-derivative](https://en.wikipedia.org/wiki/Subderivative))\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial v} = \\frac{\\partial}{\\partial v} \\text{ReLU}(v) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if}~v \\leq 0\\\\\n",
    "1 & \\text{else}\n",
    "\\end{cases}\\enspace.\n",
    "$$\n",
    "\n",
    "Hence, in our case, we have (as $v=7$)\n",
    "\n",
    "$$\\frac{\\partial a}{\\partial v} = 1$$\n",
    "\n",
    "and consequently \n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial b} = 1\\cdot 1 = 1 \\enspace.\n",
    "$$\n",
    "\n",
    "Let's try to compute the partial derivative of $a$ wrt. $w$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial u}\\frac{\\partial u}{\\partial w}\n",
    "$$\n",
    "\n",
    "We have, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial v}{\\partial u} = 1\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial w} = x\n",
    "$$\n",
    "\n",
    "Combined, we obtain (knowing that $x=3$, see figure)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial u}\\frac{\\partial u}{\\partial w} = 1\\cdot 1\\cdot 3 = 3\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Conceptually, the **forward pass** (i.e., the computation of the function value) is a sequence of standard tensor computations (i.e., in this example, simply the computation of **all** intermediate results). We need the **Directed Ascyclic Graph (DAG)** of tensor operations only to compute derivatives (via the chain rule). \n",
    "\n",
    "When executing tensor operations, PyTorch can automatically (on-the-fly) construct the graph of operations to compute the gradient of any quantity with respect to any tensor involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F # functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Let's implement the simple example from above. Note the use of `requires_grad=True` (`False` by default) for the variable `w` and `b` (explained below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.]) # Input\n",
    "w = torch.tensor([2.], requires_grad=True) \n",
    "b = torch.tensor([1.], requires_grad=True) \n",
    "\n",
    "u = x*w\n",
    "v = u+b\n",
    "a = F.relu(v)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensor has a Boolean field `requires_grad`, set to `False` by default, which\n",
    "states if PyTorch should build the graph of operations so that gradients with\n",
    "respect to it can be computed.\n",
    "\n",
    "**Note**: Only *floating point type* tensors can have their gradient computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = 7.0\n"
     ]
    }
   ],
   "source": [
    "print(\"v =\", v.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient $\\partial a/\\partial w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "grad_w = grad(a, w, retain_graph=True)\n",
    "print(grad_w[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of `retain_graph=True`. This is not required here, but in case you want to execute that cell multiple times (try it out with `retain_graph=False`), you will need it (also for executing the next call, as we call `grad` another time). The reason is that, by default, the part of the graph that computes `a` is destroyed (for memory reasons) once the gradient computation is done. If `retain_graph=False`, then execution of the next statement will fail (unless you do another forward pass). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient $\\partial a/\\partial b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(grad(a, b)[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A slightly more involved example\n",
    "\n",
    "In this example, we have a slightly more complex computation graph with a *shared* weight $w$ and two paths to arrive at the result. The function is\n",
    "\n",
    "$$f(x;w) = (wx)^2 + (wx)^3$$\n",
    "\n",
    "<img src=\"DAG1.svg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the partial derivative of $a$ wrt. $w$ by hand once more:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v_1}\\frac{\\partial v_1}{\\partial u_1}\\frac{\\partial u_1}{\\partial w} + \\frac{\\partial a}{\\partial v_2}\\frac{\\partial v_2}{\\partial u_2}\\frac{\\partial u_2}{\\partial w} = \n",
    "1\\cdot 2u_1\\cdot x + 1 \\cdot 3u_2^2 \\cdot x = 8 + 24 = 32\n",
    "$$\n",
    "\n",
    "Again, having **all the intermediate values** during the **forward pass** makes it very easy to actually compute the gradient of $a$ wrt. $w$, i.e., $\\partial a/\\partial w$.\n",
    "\n",
    "### Fix values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 12.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.])\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "u1 = x*w\n",
    "u2 = x*w\n",
    "\n",
    "v1 = torch.pow(u1, 2)\n",
    "v2 = torch.pow(u2, 3)\n",
    "\n",
    "a = v1 + v2\n",
    "print(\"a =\", a.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the gradient of $a$ wrt. $w$, i.e., $\\partial a/\\partial w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0\n"
     ]
    }
   ],
   "source": [
    "print(grad(a, w)[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The AutoGrad machinery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autograd DAG is encoded through the fields `grad_fn` of Tensors, and the\n",
    "fields `next_functions` of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "# below is another way of saying requires_grad=True\n",
    "x = torch.tensor([ 1.0, -2.0, 3.0, -4.0 ]).requires_grad_() \n",
    "a = x.pow(2.) # square each component of x\n",
    "s = a.sum()\n",
    "print(s.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we visualize a simple computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 2.]).requires_grad_()\n",
    "q = x.norm() # L2-norm of tensor sqrt(1^2+2^2+2^2)=3\n",
    "print(q.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"simple1.jpg\" align=\"center\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agtree2dot.save_dot(q, {x: 'x', q: 'q'}, open('simple1.dot', 'w'))\n",
    "HTML(visualize_DAG('simple1.dot', 'simple1.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Here's another, slightly more involved, example: We have $\\mathbf{x} \\in \\mathbb{R}^{10}$ and **first** compute (noting that $\\text{tanh}$ is applied componentwise).\n",
    "\n",
    "$$\n",
    "\\mathbf{h}  = \\text{tanh}(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) \\\\\n",
    "$$\n",
    "\n",
    "with $\\mathbf{W}_1 \\in \\mathbb{R}^{20 \\times 10}$ and $\\mathbf{b}_1 \\in \\mathbb{R}^{20}$. This results in $\\mathbf{h} \\in \\mathbb{R}^{20}$. **Second**, we compute\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}}  = \\text{tanh}(\\mathbf{W}_2\\mathbf{h} + \\mathbf{b}_2)\n",
    "$$\n",
    "\n",
    "with $\\mathbf{W}_2 \\in \\mathbb{R}^{5 \\times 20}$ and $\\mathbf{b}_2 \\in \\mathbb{R}^5$. This results in \n",
    "$\\hat{\\mathbf{y}} \\in \\mathbb{R}^5$. And **finally**, we compute (the mean-squared error)\n",
    "\n",
    "$$\n",
    "L(\\mathbf{y},\\hat{\\mathbf{y}}) = \\frac{1}{5} \\sum_{i=1}^5(y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $y_i,\\hat{y}_i$ are the $i$-th elements of $\\mathbf{y}$ and $\\hat{\\mathbf{y}}$, respectively. In the code below, $y_i$ (which we use as targets) are just random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.rand(20, 10).requires_grad_()\n",
    "b1 = torch.rand(20).requires_grad_()\n",
    "w2 = torch.rand(5, 20).requires_grad_()\n",
    "b2 = torch.rand(5).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2959361672401428\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10)\n",
    "h = torch.tanh(w1 @ x + b1)\n",
    "yhat = torch.tanh(w2 @ h + b2)\n",
    "\n",
    "y = torch.rand(5) # target\n",
    "\n",
    "L = (y - yhat).pow(2).mean()\n",
    "L.backward() # computes the partial derivatives wrt. w1,b1,w2,b2\n",
    "print(L.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the computation graph ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"simple2.jpg\" align=\"center\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agtree2dot.save_dot(L, \n",
    "                    {\n",
    "                        w1: 'w1',\n",
    "                        b1: 'b1',\n",
    "                        w2: 'w2',\n",
    "                        b2: 'b2',\n",
    "                        L: 'loss'\n",
    "                    }, open('simple2.dot', 'w'))\n",
    "HTML(visualize_DAG('simple2.dot', 'simple2.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `torch.no_grad()`\n",
    "\n",
    "The `torch.no_grad()` **context** switches off the autograd machinery, and can be\n",
    "used for operations such as parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.2000) tensor(-2.2000)\n",
      "1 tensor(0.8800) tensor(-0.8800)\n",
      "2 tensor(0.3520) tensor(-0.3520)\n",
      "3 tensor(0.1408) tensor(-0.1408)\n",
      "4 tensor(0.0563) tensor(-0.0563)\n",
      "5 tensor(0.0225) tensor(-0.0225)\n",
      "6 tensor(0.0090) tensor(-0.0090)\n",
      "7 tensor(0.0036) tensor(-0.0036)\n",
      "8 tensor(0.0014) tensor(-0.0014)\n",
      "9 tensor(0.0006) tensor(-0.0006)\n",
      "10 tensor(0.0002) tensor(-0.0002)\n",
      "11 tensor(9.2387e-05) tensor(-9.2387e-05)\n",
      "12 tensor(3.6836e-05) tensor(-3.6836e-05)\n",
      "13 tensor(1.4663e-05) tensor(-1.4663e-05)\n",
      "14 tensor(5.9605e-06) tensor(-5.9605e-06)\n",
      "15 tensor(2.3842e-06) tensor(-2.3842e-06)\n",
      "16 tensor(9.5367e-07) tensor(-9.5367e-07)\n",
      "17 tensor(3.5763e-07) tensor(-3.5763e-07)\n",
      "18 tensor(2.3842e-07) tensor(-2.3842e-07)\n",
      "19 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "20 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "21 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "22 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "23 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "24 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "25 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "26 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "27 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "28 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "29 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "30 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "31 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "32 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "33 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "34 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "35 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "36 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "37 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "38 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "39 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "40 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "41 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "42 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "43 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "44 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "45 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "46 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "47 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "48 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "49 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "50 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "51 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "52 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "53 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "54 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "55 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "56 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "57 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "58 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "59 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "60 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "61 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "62 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "63 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "64 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "65 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "66 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "67 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "68 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "69 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "70 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "71 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "72 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "73 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "74 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "75 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "76 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "77 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "78 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "79 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "80 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "81 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "82 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "83 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "84 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "85 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "86 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "87 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "88 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "89 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "90 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "91 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "92 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "93 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "94 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "95 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "96 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "97 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "98 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "99 tensor(1.1921e-07) tensor(-1.1921e-07)\n",
      "0.333333 -0.333333\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor( 0.7).requires_grad_()\n",
    "b = torch.tensor(-0.7).requires_grad_()\n",
    "eta = 0.1\n",
    "\n",
    "for k in range(100):\n",
    "    l = (a - 1)**2 + (b + 1)**2 + (a - b)**2 # i.e., the forward pass\n",
    "    ga, gb = torch.autograd.grad(l, (a, b))\n",
    "\n",
    "    print(k,ga,gb)\n",
    "    \n",
    "    # walk towards the direction of the negative gradient \n",
    "    # (aka gradient descent) - both statements will not change\n",
    "    # the computation graph, because they are within the \n",
    "    # torch.no_grad context!\n",
    "    with torch.no_grad():\n",
    "        a -= eta * ga \n",
    "        b -= eta * gb\n",
    "    \n",
    "print('%.06f' % a.item(), '%.06f' % b.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you do not believe that this is the minimum, fire up Mathematica and run the `2DMinimumExample.nb` available in this directory :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `detach()`\n",
    "\n",
    "The `detach()` method creates a tensor which shares the data, but does not\n",
    "require gradient computation, and is not connected to the current computation graph.\n",
    "\n",
    "This method should be used when the gradient should not be propagated\n",
    "beyond a variable, or to update leaf tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n",
      "tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(10, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y = x**2\n",
    "z = x**3\n",
    "r=(y+z).sum()\n",
    "print(r.item())\n",
    "r.backward() # computes the gradient of r wrt. x\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets exclude the contribution of the $x^3$ branch via `detach()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(10, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y = x**2\n",
    "z = (x**3).detach()\n",
    "r=(y+z).sum()\n",
    "print(r.item())\n",
    "r.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we have $f(x) = x^2+x^3$ and we compute $\\frac{d}{dx}f(x)$ which is $2x+3x^2$. Now, if we have $x=1$, it's clear that we see a vector of all 5's. However, if we exclude the $x^3$ computation, we end up with $2x$ which (for $x=1$ is 2) and that is what we see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting back to our exampe of finding the minimum of a function (see above) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000 -0.000000\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor( 0.5).requires_grad_()\n",
    "b = torch.tensor(-0.5).requires_grad_()\n",
    "eta = 0.1\n",
    "\n",
    "for k in range(100):\n",
    "    l = (a - 1)**2 + (b + 1)**2 + (a.detach() - b)**2\n",
    "    ga, gb = torch.autograd.grad(l, (a, b))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= eta * ga\n",
    "        b -= eta * gb\n",
    "        \n",
    "print('%.06f' % a.item(), '%.06f' % b.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 ms ± 427 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = torch.rand(500,256,256)\n",
    "b = torch.rand(500,256,256)\n",
    "torch.matmul(a,b)\n",
    "for i in range(500):\n",
    "    torch.matmul(a[i],b[i])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
