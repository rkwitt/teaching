{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision (911.908)\n",
    "\n",
    "## <font color='crimson'>Convolutions & ConvNets</font>\n",
    "\n",
    "**Changelog**:\n",
    "- *Nov. 2022*: adaptations to PyTorch 1.13 + minor fixes\n",
    "- *Nov. 2024*: adaptations to PyTorch 2.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "- [Motivation](#Motivation)\n",
    "- [Conventions & Terminology](#Conventions-and-Terminology)\n",
    "- [1D convolution](#Convolution-in-1D)\n",
    "- [2D convolution](#Convolution-in-2D)\n",
    "- [Convolution in neural networks](#Convolution-in-neural-networks)\n",
    "- [Grouped convolution](#Grouped-convolution)\n",
    "- [Resources](#Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Let's say we have input signals of large dimension, but with a very distinct inherent structure, e.g., images or acoustic signals.\n",
    "\n",
    "If we would handle these signals simply as ''vectors'', we would immediately run into problems. Just think of a relatively small $256 \\times 256$ RGB image. Mapping such an input, when considered as a vector, with a linear layer to, say, $\\mathbb{R}^{1000}$, would require a matrix of size\n",
    "\n",
    "$$1000 \\times (256\\cdot 256 \\cdot 3) = 1000 \\times 196608$$\n",
    "\n",
    "This already requires **750 MB** of memory, assuming 32-bit floating point numbers (and this would just be the memory footprint of the first layer!)\n",
    "\n",
    "Possibly even more concerning is the fact that processing such signals (e.g., images) should have some degree of invariance (e.g., wrt. translation). Put differently, a *representation meaningful at\n",
    "a certain location can / should be used everywhere*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conventions and Terminology\n",
    "\n",
    "As we know, everything in PyTorch is handled in *batches* of size `N`. A batch of size `N=1` would mean only a single input. In the context of deep learning with PyTorch, the batch size is the first dimension of a tensor.    \n",
    "\n",
    "For convolution operations, we call the second dimension the **number of channels** and any following dimensions the **channel dimensions**.\n",
    "\n",
    "A tensor of size \n",
    "`1 x 1 x 10` would mean\n",
    "\n",
    "- batch size: `1`\n",
    "- \\#channels: `1`\n",
    "- channel size: `10`\n",
    "\n",
    "A tensor of size `1 x 10 x 1` would mean\n",
    "\n",
    "- batch size: `1`\n",
    "- \\#channels: `10`\n",
    "- channel size: `1`\n",
    "\n",
    "A tensor of size `100 x 3 x 32 x 32` would mean\n",
    "\n",
    "- batch size: `100`\n",
    "- \\#channels: `3`\n",
    "- channel size: `32 x 32`\n",
    "\n",
    "A typical example of such a tensor would be 100 RGB images (i.e., 3 channels) of width and height of 32 pixels.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "We will see that convolution layers in neural networks will take input tensors of this form and output tensors of this form.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Convolution in 1D\n",
    "\n",
    "Say we have a signal $\\mathbf{x}$, written here as a row vector with $W$ elements, i.e.,\n",
    "\n",
    "$$\\mathbf{x} = [x_1,\\ldots,x_W]$$\n",
    "\n",
    "and a **convolution kernel** $\\mathbf{u}$ with $w$ elements, i.e.,\n",
    "\n",
    "$$\\mathbf{u} = [u_1,\\ldots,u_w]$$\n",
    "\n",
    "Then, *convolving* $\\mathbf{x}$ with $\\mathbf{u}$ means\n",
    "\n",
    "$$[\\mathbf{x} * \\mathbf{u}]_i = \\sum_{j=1}^w x_{i-1+j} u_j$$\n",
    "\n",
    "where $[\\mathbf{x} * \\mathbf{u}]_i$ denotes the output of the convolution operation at the $i$-th position.\n",
    "\n",
    "**Example**\n",
    "\n",
    "$$\\mathbf{x} = [1,2,3,4,5,6,7,8,9,10]$$\n",
    "\n",
    "(with $W=10$ and $w=3$) and \n",
    "\n",
    "$$\\mathbf{u} = [1,2,3]$$\n",
    "\n",
    "$$[\\mathbf{x} * \\mathbf{u}]_1 = \\sum_{j=1}^3 x_{i-1+j} u_j = x_1u_1 + x_2u_2 + x_3u_3 = 14$$\n",
    "\n",
    "where we started indexing by $1$. This is different from convolution in signal processing since we both visit signal and kernel elements in *increasing* index order.\n",
    "\n",
    "**Illustration**\n",
    "\n",
    "<img src=\"1Dconv.svg\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "The parameters of this convolution operation are the <font color='blue'>values of the convolution kernel</font>. In the previous example, we thus have 3 parameters (as we did not include bias). If we do include bias, the number of parameters would be 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input [[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]]]\n",
      "Output [[[14. 20. 26. 32. 38. 44. 50. 56.]]]\n",
      "Check (pos 0): (1*1 + 2*2 + 3*3) = 14\n",
      "Check (pos 1): (1*2 + 2*3 + 3*4) = 20\n",
      "torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# create toy input [1,2,3,4,...,10] \n",
    "x = torch.tensor(list(np.arange(1, 11)), dtype=torch.float32)\n",
    "\n",
    "# view the input as a 1x1x10 tensor, i.e., batch-size 1, 1x10 inputs\n",
    "x = x.view(1, 1, 10)\n",
    "\n",
    "# 1D convolution\n",
    "conv_layer = nn.Conv1d(in_channels=1,   # one input channel\n",
    "              out_channels=1,  # one output channel\n",
    "              kernel_size=3,   # use a kernel size of 3\n",
    "              stride=1,        # move the kernel along by steps of 1\n",
    "              padding=0,       # do not pad the input\n",
    "              bias=False)      # do not include bias (i.e., the b in Ax+b)\n",
    "\n",
    "# directly set the convolution kernel parameters (for demonstration)\n",
    "conv_layer.weight.data = torch.tensor([1, 2, 3], dtype=torch.float32).view(1, 1, 3)\n",
    "\n",
    "# forward through the 1D conv. operation\n",
    "y = conv_layer(x)\n",
    "\n",
    "print('Input', x.numpy())\n",
    "print('Output', y.detach().numpy())\n",
    "\n",
    "# Note that the formula from above also works when we \n",
    "# start indexing by 0\n",
    "print('Check (pos 0): (1*1 + 2*2 + 3*3) = 14')\n",
    "print('Check (pos 1): (1*2 + 2*3 + 3*4) = 20')\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that a linear layer could implement the *same* operation here, but we would need to be careful when we compute gradient updates, as we would have to make sure that the *zero* entries stay zero (can be done via masking for instance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding**\n",
    "\n",
    "Per default, *padding* is set to zero, so the output (in the previous example) is of size `1 x 1 x 8`; in other words, a kernel of size 3 with a stride of 1 can be applied 8 times.\n",
    "Next, let's pad with *one zero entry* on both sides and still use a stride of 1; in this setting, we **preserve the input dimension**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(list(np.arange(1,11)), dtype=torch.float32).view(1,1,10)\n",
    "conv_layer = nn.Conv1d(in_channels=1, \n",
    "              out_channels=1, \n",
    "              kernel_size=3, \n",
    "              stride=1, \n",
    "              padding=1, #!!!!! \n",
    "              bias=False) \n",
    "y = conv_layer(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple convolution kernels**\n",
    "\n",
    "We do not have to use just one kernel, we can use as many as we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(list(np.arange(1, 11)), dtype=torch.float32).view(1, 1, 10)\n",
    "\n",
    "m = nn.Conv1d(in_channels=1, \n",
    "              out_channels=5, \n",
    "              kernel_size=3, \n",
    "              stride=1, \n",
    "              padding=0, \n",
    "              bias=False)\n",
    "y = m(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 0.2249, -0.0333,  0.4590]],\n",
       "\n",
       "        [[ 0.3880, -0.2911,  0.5647]],\n",
       "\n",
       "        [[-0.5703, -0.0178, -0.5172]],\n",
       "\n",
       "        [[ 0.2905,  0.1148, -0.1274]],\n",
       "\n",
       "        [[-0.0505,  0.0906, -0.2393]]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we not only convolve with one kernel but with **5 (different) kernels**. Hence, we obtain five output channels of size 10. The parameter tensor of the 1D convolution layer from the last example is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(m.weight.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, 5 kernels of size $1 \\times 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stride**\n",
    "\n",
    "Let's not shift the kernel by one position a time, but by two. This effectively *downsamples* the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])\n",
      "tensor([[[ 1.6212,  2.8230,  4.0249,  5.2267],\n",
      "         [ 1.1897,  1.4633,  1.7369,  2.0105],\n",
      "         [-1.7915, -3.4791, -5.1668, -6.8544],\n",
      "         [ 2.7739,  5.4639,  8.1538, 10.8438],\n",
      "         [ 1.3648,  2.8976,  4.4303,  5.9631]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([1, 5, 4])\n",
      "#Parameters:  15\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(list(np.arange(1, 11)), dtype=torch.float32).view(1, 1, 10)\n",
    "\n",
    "m = nn.Conv1d(in_channels=1, \n",
    "              out_channels=5,\n",
    "              kernel_size=3, \n",
    "              stride=2, \n",
    "              padding=0, \n",
    "              bias=False)\n",
    "y = m(x)\n",
    "print(x)\n",
    "print(y)\n",
    "print(y.size())\n",
    "\n",
    "# check the number of parameters - striding does obviously not change anything\n",
    "n_params = 0\n",
    "for p in m.parameters(): \n",
    "   n_params += p.numel()\n",
    "print('#Parameters: ', n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Bias.</b> In all previous examples, we disabled the bias term. Per default, this is enabled. If so, we have one additional parameter. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4])\n",
      "#Parameters:  4\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(list(np.arange(1, 11)), dtype=torch.float32).view(1, 1, 10)\n",
    "m = nn.Conv1d(1, 1, 3, stride=2, padding=0, bias=True)\n",
    "y = m(x)\n",
    "print(y.size())\n",
    "\n",
    "n_params = 0\n",
    "for p in m.parameters(): \n",
    "    n_params += p.numel()\n",
    "print('#Parameters: ', n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Also note that at all positions where we apply the kernel, we use the same kernel parameters. We call this <b>weight sharing</b>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution as a linear operation\n",
    "\n",
    "In fact, it's fairly easy to see that a convolution operation (without bias in our example) can be implemented via a linear/full-connected layer. In the following visualizations, gray dots (at the bottom figure) indicate zero values. \n",
    "\n",
    "<img src=\"WeightSharing.svg\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass through a PyTorch 1D convolution layer\n",
      "[14. 20. 26.]\n",
      "\n",
      "Forward pass through a corresponding PyTorch linear layer\n",
      "[14. 20. 26.]\n"
     ]
    }
   ],
   "source": [
    "# input is a 1x5 tensor\n",
    "x_inp = torch.tensor([[1.,2.,3.,4.,5.]])\n",
    "\n",
    "# 1D convolution layer, 1 input channel, 1 output channel, kernel size 3, no bias\n",
    "layer = nn.Conv1d(1,1,3,bias=False)\n",
    "layer.weight.data = torch.tensor([[[1.,2.,3.]]])\n",
    "print('Forward pass through a PyTorch 1D convolution layer')\n",
    "print(layer(x_inp.unsqueeze(0)).detach().squeeze().numpy())\n",
    "print()\n",
    "\n",
    "# let's implement the same mapping using a linear layer using the following \n",
    "# weight matrix:\n",
    "A = torch.tensor([\n",
    "    [1.,2.,3.,0.,0.],\n",
    "    [0.,1.,2.,3.,0.],\n",
    "    [0.,0.,1.,2.,3.]\n",
    "])\n",
    "\n",
    "lin_layer = nn.Linear(5, 3, bias=False)\n",
    "lin_layer.weight.data = A\n",
    "print('Forward pass through a corresponding PyTorch linear layer')\n",
    "print(lin_layer(x_inp).detach().squeeze().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Convolution in 2D\n",
    "\n",
    "We will not discuss 2D convolution formally, as the specification of the convolution operation becomes tedious. The principle is quite simple and remains the same as in the 1D case. \n",
    "\n",
    "In the following example, we have an input tensor of size `W x H` (width times height) and `C` channels. The convolution kernel has spatial size `w x h` and also `C` channels.\n",
    "\n",
    "Convolution with this kernel (using `stride=1` and *no* padding) gives a tensor of size `1 x (W-w+1) x (H-h+1)`. If we use 2 kernels, we get, as output, a tensor of size `2 x (W-w+1) x (H-h+1)`, etc.\n",
    "\n",
    "<img src=\"2Dconv.svg\" style=\"width: 350px;\"/>\n",
    "\n",
    "**Parameters**: The number of parameters in the first case is `w*h*C` (or `w*h*C+C` if we include the bias). We see that **weight sharing** allows us to efficiently process fairly large inputs with relatively few parameters.  \n",
    "\n",
    "Also note that in case we use `K` kernels, the parameters of the convolution layers are stored in a `K x C x w x h` tensor. If bias is included we also have an `K` additional parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 15, 15])\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(16,50,32,32) # 16 inputs of size 50x32x32 (32x32 with 50 channels)\n",
    "\n",
    "# 2D convolution layer\n",
    "c1 = nn.Conv2d(\n",
    "    in_channels=50,\n",
    "    stride=2,\n",
    "    out_channels=10, # nr. of conv. kernels\n",
    "    kernel_size=3,   # short form for 3x3\n",
    "    padding=0,       # padding with zeros \n",
    "    bias=False)\n",
    "\n",
    "out = c1(x)\n",
    "print(out.shape)\n",
    "\n",
    "s = 0\n",
    "print(np.sum([p.numel() for p in c1.parameters()]))\n",
    "\n",
    "\n",
    "# c1 = nn.Conv2d(3,10,kernel_size=3,bias=False)\n",
    "# out1 = c1(x)\n",
    "# print(out1.shape)\n",
    "# c2 = nn.Conv2d(10,100,stride=1,kernel_size=7,bias=False)\n",
    "# out2 = c2(out1)\n",
    "# print(out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 3, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.weight.shape # we have 10 kernels with 50 channels each of size 3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter weight : #parameters = 27\n",
      "---\n",
      "Parameter weight : #parameters = 27\n",
      "Parameter bias : #parameters = 3\n"
     ]
    }
   ],
   "source": [
    "K = 3 # 3 kernels / output channels\n",
    "m_wo_bias = nn.Conv2d(1, K, 3, stride=2, padding=0, bias=False) # 3x3 kernels\n",
    "m_w_bias = nn.Conv2d(1, K, 3, stride=2, padding=0, bias=True)   # 3x3 kernels\n",
    "\n",
    "for (name, p) in m_wo_bias.named_parameters():\n",
    "    print('Parameter {} : #parameters = {}'.format(name, p.numel()))\n",
    "print('---')\n",
    "for (name, p) in m_w_bias.named_parameters():\n",
    "    print('Parameter {} : #parameters = {}'.format(name, p.numel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology**: When processing inputs with convolution layers having $K$ kernels, we produce $K$ outputs (i.e., the `output_channels`). We sometimes also say, we produce $K$ **feature maps**. This makes sense, if we consider, e.g., a $5 \\times 5$ kernel as identifying relevant features and applying this kernel to the full input produces a **feature map** (just think about an edge detection filter for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 15, 15])\n",
      "#Parameters:  55\n"
     ]
    }
   ],
   "source": [
    "## x = torch.randn(10, 6, 32, 32)\n",
    "m = nn.Conv2d(6, 1, 3, stride=2, padding=0, bias=True)\n",
    "print(m(x).size())\n",
    "\n",
    "n_params = 0\n",
    "for p in m.parameters(): \n",
    "    n_params += p.numel()\n",
    "print('#Parameters: ', n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we note that the input tensor is of size `10 x 6 x 32 x 32`, i.e., \n",
    "\n",
    "- batch size: `10`\n",
    "- #channels: `6`\n",
    "- channel size: `32 x 32`\n",
    "\n",
    "Our kernel will have size `1 x 6 x 3 x 3`, so `6` channels of size `3 x 3`; the number of channels corresponds to the channels of the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution vs. a linear layer** (aka fully-connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec torch.Size([10, 6144])\n",
      "o_vec torch.Size([10, 900])\n"
     ]
    }
   ],
   "source": [
    "x_vec = x.view(10,-1) # vectorize the input, i.e.,\n",
    "o_vec = m(x).view(10,-1)\n",
    "\n",
    "print('x_vec', x_vec.size())\n",
    "print('o_vec', o_vec.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To map the input tensor to the same output (as with our convolution), we would need a mapping\n",
    "\n",
    "$$f: \\mathbb{R}^{6144} \\to \\mathbb{R}^{900}$$\n",
    "\n",
    "i.e., a matrix $\\mathbf{W}$ of size `6144 x 900` having `5529600` parameters! (our conv. layer only has 54 parameters, or 55 with bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Convolution in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cl0 = nn.Conv2d( 3,  32,  3, 2, 0)\n",
    "        self.cl1 = nn.Conv2d(32,  64,  3, 2, 0)\n",
    "        self.cl2 = nn.Conv2d(64, 128,  3, 2, 0)\n",
    "        self.cl3 = nn.Conv2d(128, 256, 3, 2, 0)\n",
    "        self.cl4 = nn.Conv2d(256, 512, 3, 1, 0)\n",
    "        self.lin = nn.Linear(512*5*4,10) # maps to 10 output classes\n",
    "        \n",
    "    def forward(self, x): # x is input\n",
    "        x = F.relu(self.cl0(x))\n",
    "        x = F.relu(self.cl1(x))\n",
    "        x = F.relu(self.cl2(x))\n",
    "        x = F.relu(self.cl3(x))\n",
    "        x = F.relu(self.cl4(x))\n",
    "        x = self.lin(x.view(-1,512*5*4))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.0418e-02,  5.5185e-02, -4.0842e-03],\n",
       "          [-2.1585e-02,  3.0093e-02, -3.4872e-02],\n",
       "          [-2.6062e-02, -5.8427e-02,  1.7096e-02]],\n",
       "\n",
       "         [[-1.6737e-02, -4.0587e-02,  3.9432e-02],\n",
       "          [ 1.2708e-02,  6.6133e-02, -2.2328e-02],\n",
       "          [-2.5602e-02,  4.3332e-02, -3.7078e-02]],\n",
       "\n",
       "         [[-6.9003e-03, -2.5788e-02, -2.6211e-02],\n",
       "          [ 4.9722e-03, -9.2649e-03,  9.0847e-03],\n",
       "          [ 3.7069e-02,  1.2016e-02,  1.5236e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 9.1729e-02,  1.5076e-02, -3.4769e-02],\n",
       "          [-9.9562e-03,  5.5335e-02,  3.8469e-02],\n",
       "          [-5.6177e-02, -7.2462e-03, -1.4704e-02]],\n",
       "\n",
       "         [[ 1.4355e-02,  8.5070e-03, -1.5027e-02],\n",
       "          [ 4.2403e-02,  2.0795e-02, -3.8695e-02],\n",
       "          [ 5.8821e-03, -2.0124e-02,  1.6507e-02]],\n",
       "\n",
       "         [[ 3.0216e-02,  3.2039e-02, -8.9484e-03],\n",
       "          [ 1.5327e-02,  2.8527e-02, -4.5960e-03],\n",
       "          [ 1.2244e-02, -1.0012e-03,  2.8863e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.9378e-02,  9.6142e-03, -4.8680e-02],\n",
       "          [ 7.8899e-03, -9.9829e-03, -4.5257e-02],\n",
       "          [ 3.5061e-03,  2.4436e-02,  1.5814e-02]],\n",
       "\n",
       "         [[-1.3765e-02, -4.0836e-02, -1.5888e-02],\n",
       "          [-2.1398e-02, -2.0534e-02, -2.4339e-02],\n",
       "          [ 3.2541e-02, -1.8650e-02,  2.3547e-02]],\n",
       "\n",
       "         [[ 1.3460e-02, -2.0197e-02,  3.3846e-02],\n",
       "          [-1.1539e-02,  6.3384e-03,  1.9877e-02],\n",
       "          [ 5.7105e-03, -2.4735e-03,  3.0407e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.1815e-02, -1.2405e-02, -3.9358e-02],\n",
       "          [-1.2162e-02,  5.8369e-03,  2.1578e-02],\n",
       "          [ 1.9890e-02, -1.1624e-02,  3.7004e-02]],\n",
       "\n",
       "         [[-4.5355e-02,  4.8097e-02, -1.4056e-02],\n",
       "          [-4.6276e-03,  4.6793e-03,  4.0686e-02],\n",
       "          [ 1.1732e-02,  2.5239e-02,  4.7129e-02]],\n",
       "\n",
       "         [[-4.0492e-03, -1.4525e-02,  1.3938e-02],\n",
       "          [-7.1346e-03,  1.0425e-02,  1.7693e-02],\n",
       "          [-1.6116e-02,  5.5761e-02, -4.7312e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6997e-02,  4.2763e-02,  5.1198e-02],\n",
       "          [-2.8256e-02, -5.7406e-03, -2.0837e-02],\n",
       "          [ 1.0603e-02,  4.7807e-02, -1.8793e-02]],\n",
       "\n",
       "         [[ 3.2702e-02,  1.6388e-02, -7.5726e-04],\n",
       "          [-3.2662e-02, -2.9630e-02, -1.3041e-02],\n",
       "          [-1.6277e-02, -3.3706e-02, -2.2556e-02]],\n",
       "\n",
       "         [[ 1.4889e-02,  1.9051e-02, -8.1390e-03],\n",
       "          [ 2.8699e-02, -2.3742e-02, -1.6639e-03],\n",
       "          [-2.5683e-02, -4.2717e-02, -2.4968e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 4.6133e-02, -2.5827e-02,  3.5295e-02],\n",
       "          [ 7.5928e-02, -2.5460e-02,  1.4962e-02],\n",
       "          [-4.9674e-02, -1.7294e-02,  2.6510e-02]],\n",
       "\n",
       "         [[ 1.9387e-02,  2.5375e-02,  2.5712e-02],\n",
       "          [ 4.5234e-02, -5.5029e-02,  6.7782e-03],\n",
       "          [-2.8701e-02, -1.3530e-02, -3.5245e-02]],\n",
       "\n",
       "         [[-3.2269e-02,  1.8757e-02, -3.1460e-02],\n",
       "          [ 3.4540e-02, -2.1540e-03, -1.2241e-02],\n",
       "          [ 1.4188e-02, -1.8852e-02,  5.0281e-03]]],\n",
       "\n",
       "\n",
       "        [[[-5.1325e-02,  1.4944e-02,  2.1847e-02],\n",
       "          [-3.4153e-02, -2.5900e-02,  5.7581e-02],\n",
       "          [-1.6925e-02,  3.4414e-02,  3.4691e-02]],\n",
       "\n",
       "         [[-1.1902e-02,  6.0228e-03,  7.1305e-03],\n",
       "          [ 1.2605e-02,  4.6175e-02, -2.8876e-02],\n",
       "          [ 3.0561e-04, -4.7542e-03,  9.3309e-03]],\n",
       "\n",
       "         [[-1.2382e-02, -1.7928e-02, -1.0311e-02],\n",
       "          [ 6.8940e-02,  3.7873e-02,  2.6465e-02],\n",
       "          [-2.5099e-02, -7.8553e-02,  3.2980e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.1127e-02, -1.9886e-02,  3.8599e-02],\n",
       "          [ 2.1616e-02,  1.9477e-03, -9.1760e-02],\n",
       "          [ 3.9698e-02,  2.1587e-02, -6.5216e-03]],\n",
       "\n",
       "         [[-2.9140e-02,  2.9851e-02,  2.3088e-02],\n",
       "          [ 4.5581e-02,  5.2730e-03, -9.7531e-03],\n",
       "          [ 2.9980e-02, -4.2891e-02, -2.9455e-02]],\n",
       "\n",
       "         [[-4.1820e-02,  1.2774e-02,  5.6053e-02],\n",
       "          [-3.3706e-02,  1.9209e-02,  1.3457e-02],\n",
       "          [-3.3189e-02, -1.6692e-02,  6.5387e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.5501e-02, -4.8029e-02, -3.5604e-02],\n",
       "          [-2.8638e-02,  2.3990e-02, -2.7801e-02],\n",
       "          [-2.5786e-02, -4.2886e-02, -6.3606e-02]],\n",
       "\n",
       "         [[ 4.4586e-03,  1.6574e-03, -1.5800e-02],\n",
       "          [ 2.1514e-02, -1.3478e-02, -8.1874e-03],\n",
       "          [ 5.0469e-02,  2.1366e-02,  1.9963e-02]],\n",
       "\n",
       "         [[-8.7643e-03, -1.0294e-02, -1.0175e-02],\n",
       "          [-2.2493e-02,  1.2332e-02, -9.7546e-03],\n",
       "          [-8.8586e-03, -1.0970e-02, -2.8700e-02]]],\n",
       "\n",
       "\n",
       "        [[[-5.1051e-03, -1.7087e-02, -9.4111e-03],\n",
       "          [ 7.9678e-02,  6.3042e-05,  2.7752e-02],\n",
       "          [-4.6261e-02,  1.9111e-02,  1.8889e-02]],\n",
       "\n",
       "         [[-3.8384e-03,  3.2939e-02,  6.8064e-02],\n",
       "          [ 1.3673e-02,  1.6265e-02,  2.6213e-02],\n",
       "          [-1.1898e-02,  1.8943e-02,  7.4751e-03]],\n",
       "\n",
       "         [[ 3.3620e-02, -7.9801e-03, -6.8206e-03],\n",
       "          [ 3.9513e-02,  1.4675e-02,  1.4278e-03],\n",
       "          [ 1.7025e-02, -4.6771e-02, -1.7090e-02]]],\n",
       "\n",
       "\n",
       "        [[[-6.7121e-02,  7.2578e-03, -3.0024e-02],\n",
       "          [ 7.1171e-02,  8.0840e-03,  3.4276e-02],\n",
       "          [-1.6439e-03,  1.3416e-02, -5.2643e-02]],\n",
       "\n",
       "         [[ 2.8374e-02,  4.2483e-02,  4.6301e-03],\n",
       "          [ 6.6481e-02, -2.8825e-02,  3.8417e-02],\n",
       "          [ 4.8107e-02,  4.6026e-02, -3.0273e-02]],\n",
       "\n",
       "         [[ 9.9237e-03, -9.0943e-04,  1.9181e-02],\n",
       "          [-5.6955e-03,  2.3690e-02,  1.1035e-02],\n",
       "          [ 3.1822e-03,  6.0761e-03, -7.5539e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.3744e-02, -1.8904e-02,  5.2976e-02],\n",
       "          [-2.9937e-02, -1.9542e-02, -9.3713e-04],\n",
       "          [-2.0487e-02, -1.5321e-02, -2.5035e-02]],\n",
       "\n",
       "         [[-2.5091e-04,  2.6483e-02, -3.7514e-02],\n",
       "          [-8.3316e-03, -1.7638e-02, -5.3493e-02],\n",
       "          [ 3.6759e-03,  1.2397e-02, -3.5398e-03]],\n",
       "\n",
       "         [[-3.9571e-02, -1.3768e-02,  7.3177e-03],\n",
       "          [ 1.6871e-02, -7.2884e-03,  5.3919e-02],\n",
       "          [-3.7669e-02, -5.5942e-02, -2.1625e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2499e-02, -6.4760e-02,  3.6500e-03],\n",
       "          [-2.6449e-02, -3.4060e-02,  2.8039e-02],\n",
       "          [ 3.3860e-03,  1.6979e-02, -3.4749e-02]],\n",
       "\n",
       "         [[ 2.7516e-02, -5.4137e-02, -3.4345e-02],\n",
       "          [ 5.7705e-02,  9.7821e-03, -6.1261e-02],\n",
       "          [-2.4798e-02, -3.8370e-02, -3.9853e-02]],\n",
       "\n",
       "         [[-1.8472e-02, -4.3717e-02, -5.3163e-03],\n",
       "          [ 3.4913e-02, -3.0713e-02, -1.2050e-02],\n",
       "          [ 1.6649e-02,  4.8902e-02, -2.1766e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.4529e-02, -3.2971e-02,  1.9801e-02],\n",
       "          [-3.6857e-02,  7.7847e-02,  3.0435e-02],\n",
       "          [-4.1985e-04,  1.2368e-02, -4.3614e-02]],\n",
       "\n",
       "         [[-2.2821e-02,  1.3637e-02,  4.3676e-02],\n",
       "          [ 6.1750e-02,  3.5241e-02,  3.2404e-02],\n",
       "          [ 1.7123e-02, -8.2628e-03,  6.4353e-02]],\n",
       "\n",
       "         [[ 9.1002e-02,  4.0498e-02,  2.2281e-02],\n",
       "          [ 1.4793e-02, -7.1693e-03,  1.5466e-02],\n",
       "          [ 1.0209e-02, -1.6721e-02,  1.3316e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.7760e-03,  1.5330e-03,  2.4657e-02],\n",
       "          [ 2.0121e-02,  3.2065e-02, -2.0301e-02],\n",
       "          [ 4.5857e-02, -4.5130e-02, -4.9436e-02]],\n",
       "\n",
       "         [[-2.3865e-02, -7.8103e-02,  2.5609e-02],\n",
       "          [-2.9279e-02, -1.8248e-02, -1.7739e-02],\n",
       "          [-2.2717e-02,  7.2708e-03, -4.7351e-02]],\n",
       "\n",
       "         [[ 2.4613e-02,  7.4118e-04,  2.9555e-02],\n",
       "          [-2.6638e-03,  2.3485e-02, -4.9770e-02],\n",
       "          [ 2.1419e-03, -2.9635e-02, -2.5279e-02]]],\n",
       "\n",
       "\n",
       "        [[[-4.2186e-03,  2.3799e-02,  1.1127e-02],\n",
       "          [ 1.0838e-02, -1.3494e-02, -2.3085e-02],\n",
       "          [-2.2841e-03,  1.2287e-02, -1.9225e-02]],\n",
       "\n",
       "         [[ 3.2262e-02,  2.4993e-02,  2.4433e-02],\n",
       "          [-2.1987e-02, -1.5754e-02, -2.1434e-02],\n",
       "          [-1.5645e-03, -2.8453e-03, -2.7312e-02]],\n",
       "\n",
       "         [[-1.5273e-02, -1.1265e-03, -3.0994e-02],\n",
       "          [ 9.3447e-03, -3.5481e-02,  1.6864e-02],\n",
       "          [ 1.2054e-02, -3.2879e-02,  2.2267e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.7935e-02, -1.7735e-02, -2.8015e-02],\n",
       "          [ 3.5999e-02,  1.0879e-02,  1.6658e-02],\n",
       "          [-4.7170e-04, -1.1676e-02, -1.5099e-02]],\n",
       "\n",
       "         [[ 7.0660e-03, -1.8384e-02, -1.1471e-02],\n",
       "          [-1.6540e-03, -2.6108e-02, -5.0554e-02],\n",
       "          [-1.3096e-02, -1.3652e-02, -1.4240e-02]],\n",
       "\n",
       "         [[-7.4621e-02, -6.4617e-03,  8.4972e-03],\n",
       "          [ 6.1265e-02,  1.4574e-02,  1.9283e-02],\n",
       "          [-3.3248e-02,  9.4911e-04, -2.8346e-02]]],\n",
       "\n",
       "\n",
       "        [[[-8.7341e-03,  4.3457e-02, -7.6016e-03],\n",
       "          [ 1.9982e-02, -9.2641e-04,  4.9736e-03],\n",
       "          [-1.0827e-02,  9.6240e-03, -2.0825e-02]],\n",
       "\n",
       "         [[-1.7574e-02,  3.2490e-02, -7.1470e-04],\n",
       "          [ 2.7651e-02, -1.0330e-02,  3.2990e-02],\n",
       "          [-1.5138e-02, -4.3628e-02,  2.2556e-02]],\n",
       "\n",
       "         [[ 2.4094e-02,  8.7772e-03, -1.2833e-02],\n",
       "          [ 3.9535e-02,  3.6278e-02, -1.8891e-02],\n",
       "          [ 2.2494e-02,  7.3137e-03,  1.5129e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.1889e-02, -1.0762e-02, -3.7266e-02],\n",
       "          [ 1.1048e-02, -4.7879e-03, -1.8762e-02],\n",
       "          [-7.0376e-03,  2.6590e-02,  1.3447e-02]],\n",
       "\n",
       "         [[ 1.1520e-02, -2.0953e-02, -7.9617e-02],\n",
       "          [-1.9425e-02,  1.2866e-02,  1.8741e-02],\n",
       "          [ 3.4033e-02, -2.4700e-02, -1.8118e-02]],\n",
       "\n",
       "         [[ 2.4416e-02, -6.1778e-02, -2.1392e-02],\n",
       "          [ 1.7550e-02, -3.4198e-02, -1.1915e-02],\n",
       "          [-2.5483e-02,  1.7568e-02,  1.6563e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8903e-02,  4.5814e-02,  3.1246e-02],\n",
       "          [ 2.9579e-02,  7.0467e-02,  3.1254e-02],\n",
       "          [ 1.9007e-02, -2.3945e-02, -1.5205e-02]],\n",
       "\n",
       "         [[-3.8585e-03,  3.0746e-02, -3.3588e-02],\n",
       "          [-4.7180e-02, -5.0063e-03, -1.2082e-03],\n",
       "          [-4.7610e-03, -8.0256e-03, -6.7300e-04]],\n",
       "\n",
       "         [[-2.0779e-02,  8.2793e-02, -2.1841e-02],\n",
       "          [ 8.5020e-03, -1.9116e-02,  1.0110e-03],\n",
       "          [-2.5373e-02, -8.2301e-03,  3.8514e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0356e-02,  1.5275e-03,  4.3290e-02],\n",
       "          [ 1.5893e-02, -1.9286e-02, -4.0368e-03],\n",
       "          [ 1.1093e-01, -7.7419e-03, -5.5553e-02]],\n",
       "\n",
       "         [[ 3.1180e-02,  1.2010e-02,  4.0467e-02],\n",
       "          [ 4.4307e-02, -1.1052e-03, -3.1687e-02],\n",
       "          [ 2.8960e-02, -3.8784e-02,  2.9025e-02]],\n",
       "\n",
       "         [[ 1.0106e-02,  3.3987e-02, -1.0899e-02],\n",
       "          [-1.0449e-02, -8.5156e-03, -8.5754e-03],\n",
       "          [-5.6246e-03, -1.3388e-02,  4.3906e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.7965e-02, -2.1758e-02, -3.5810e-02],\n",
       "          [-8.3543e-03, -1.8598e-02,  2.3503e-02],\n",
       "          [ 2.0363e-02, -5.9205e-02,  2.6223e-02]],\n",
       "\n",
       "         [[ 5.0682e-02,  1.1994e-02,  4.6101e-02],\n",
       "          [-1.0922e-02,  1.1104e-02, -2.8694e-02],\n",
       "          [ 1.3541e-03,  3.0429e-02,  6.2777e-02]],\n",
       "\n",
       "         [[-1.5150e-02, -1.8537e-02,  9.8875e-03],\n",
       "          [-7.2094e-03, -4.4389e-02,  1.4839e-02],\n",
       "          [-5.5505e-02, -1.0716e-03,  3.3878e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.9029e-02,  2.2180e-03,  5.1801e-02],\n",
       "          [-4.1548e-03, -9.5478e-04, -3.3131e-03],\n",
       "          [-1.9373e-02, -3.8889e-02, -3.3982e-02]],\n",
       "\n",
       "         [[-1.0745e-02, -2.1472e-02, -4.1084e-02],\n",
       "          [ 4.3953e-02,  2.6388e-02,  4.2400e-02],\n",
       "          [ 4.9393e-02,  5.4606e-02,  1.2825e-02]],\n",
       "\n",
       "         [[ 4.7542e-02, -7.2378e-03,  4.6505e-02],\n",
       "          [ 5.0327e-03,  8.4475e-03,  3.7800e-04],\n",
       "          [ 3.0744e-02, -9.0136e-03,  1.1732e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 7.7650e-03,  6.8456e-02,  1.6038e-02],\n",
       "          [ 4.6298e-02,  2.6200e-02,  4.0282e-02],\n",
       "          [-9.5196e-03,  1.3975e-02, -3.7417e-02]],\n",
       "\n",
       "         [[-1.9041e-02, -1.9919e-02, -2.4762e-02],\n",
       "          [ 2.0494e-02,  1.9080e-02,  3.4446e-03],\n",
       "          [ 8.8274e-03, -1.4294e-02,  5.4763e-03]],\n",
       "\n",
       "         [[-2.5824e-02,  8.7907e-03, -4.1982e-03],\n",
       "          [ 1.0386e-02, -1.0240e-02, -4.3359e-02],\n",
       "          [ 5.0729e-03, -2.2371e-02,  2.1540e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.0832e-02,  2.2930e-02,  9.7379e-03],\n",
       "          [ 6.2068e-02,  2.6936e-02, -7.2480e-03],\n",
       "          [ 5.3214e-03,  1.8984e-02, -1.6287e-02]],\n",
       "\n",
       "         [[-8.7526e-02, -2.7613e-02,  3.0499e-02],\n",
       "          [-4.1520e-02,  3.4855e-02,  5.0526e-03],\n",
       "          [ 7.3827e-03, -2.9048e-03, -2.3973e-02]],\n",
       "\n",
       "         [[ 6.1886e-02,  1.7951e-02, -5.0070e-03],\n",
       "          [ 2.8776e-02, -3.5497e-02,  2.8529e-02],\n",
       "          [-2.3639e-02,  1.3232e-02, -4.1649e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.2456e-02,  9.1062e-03, -8.1280e-02],\n",
       "          [-1.8201e-02, -6.7727e-02, -2.7250e-02],\n",
       "          [ 5.5421e-02,  5.1766e-03,  8.5497e-02]],\n",
       "\n",
       "         [[-2.4377e-02,  6.1051e-02, -5.4783e-02],\n",
       "          [ 2.0781e-02,  1.4869e-02,  3.2583e-02],\n",
       "          [ 3.6256e-02,  2.0849e-02, -1.3465e-02]],\n",
       "\n",
       "         [[ 2.9970e-02,  6.3502e-04,  8.5961e-03],\n",
       "          [-8.2716e-03,  6.9954e-02,  1.3998e-02],\n",
       "          [ 2.0638e-02, -2.9285e-02, -1.6199e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 8.7658e-03,  6.3108e-03, -1.6627e-02],\n",
       "          [-4.0978e-02,  1.5913e-02,  4.9643e-02],\n",
       "          [ 2.7994e-02, -7.6330e-02, -3.9745e-02]],\n",
       "\n",
       "         [[-2.9207e-02, -3.9870e-04, -1.5940e-02],\n",
       "          [ 1.7433e-02, -1.0783e-02,  3.3877e-03],\n",
       "          [-5.4587e-02, -8.9624e-03, -1.0868e-01]],\n",
       "\n",
       "         [[ 3.0451e-03,  6.2365e-02,  4.1556e-02],\n",
       "          [ 4.2589e-02, -1.1209e-02,  7.4969e-03],\n",
       "          [-3.3298e-02, -2.7041e-02,  7.3803e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7255e-02, -9.7492e-04, -1.1285e-02],\n",
       "          [-1.5291e-02,  3.8815e-02, -1.1054e-02],\n",
       "          [-1.3438e-02,  6.7620e-02, -3.1523e-02]],\n",
       "\n",
       "         [[-3.8593e-02, -1.3023e-02,  5.7939e-02],\n",
       "          [ 9.6990e-02,  3.4943e-02,  9.2016e-02],\n",
       "          [-4.4101e-02, -4.3953e-02, -1.7745e-02]],\n",
       "\n",
       "         [[-1.6469e-02,  3.0743e-02, -1.3184e-02],\n",
       "          [ 2.7599e-02,  2.3446e-02,  2.4978e-04],\n",
       "          [-3.1014e-02, -1.6704e-02, -1.9361e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 7.2085e-04,  8.4400e-03,  2.6043e-02],\n",
       "          [-4.9897e-02,  1.8387e-03,  3.5260e-04],\n",
       "          [-5.2491e-03, -7.3928e-02,  1.2316e-02]],\n",
       "\n",
       "         [[-2.9827e-02, -3.2663e-02,  1.8970e-02],\n",
       "          [ 3.4223e-02, -1.2050e-02,  1.3441e-02],\n",
       "          [ 2.1862e-02,  1.7337e-02,  2.8657e-02]],\n",
       "\n",
       "         [[-7.2637e-02, -4.8561e-02,  4.2028e-02],\n",
       "          [-2.4832e-02,  6.2161e-02,  1.7100e-02],\n",
       "          [ 9.8159e-03,  2.5493e-02,  3.5053e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.2735e-02,  7.4634e-02,  3.2033e-02],\n",
       "          [ 4.8670e-02, -1.7856e-04, -4.2508e-03],\n",
       "          [ 4.8410e-02, -4.0489e-02, -3.8148e-02]],\n",
       "\n",
       "         [[ 3.4237e-02,  2.2823e-02,  4.0946e-02],\n",
       "          [-1.8354e-02, -4.1584e-02,  4.8374e-02],\n",
       "          [ 1.5391e-02,  9.8834e-02, -1.8253e-02]],\n",
       "\n",
       "         [[ 2.3141e-02, -4.7421e-03, -2.5430e-02],\n",
       "          [-2.8314e-02,  4.4490e-02, -8.7208e-03],\n",
       "          [ 1.2057e-02, -7.1807e-02,  3.0361e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.6465e-02,  2.2286e-02,  2.3357e-02],\n",
       "          [ 3.2455e-02, -1.1817e-02, -5.7828e-03],\n",
       "          [ 1.0109e-02, -2.4629e-02,  1.4221e-02]],\n",
       "\n",
       "         [[ 1.3222e-02,  2.9269e-02, -2.1308e-02],\n",
       "          [-2.3294e-02,  3.3366e-02,  2.6543e-02],\n",
       "          [-1.6187e-02, -5.5391e-02, -6.8420e-02]],\n",
       "\n",
       "         [[ 7.6569e-03, -3.4306e-02, -1.3432e-02],\n",
       "          [-1.0298e-02,  3.1263e-02,  4.4380e-03],\n",
       "          [ 5.8295e-03,  5.4588e-02, -1.9239e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.0877e-02,  3.2061e-02,  1.4150e-02],\n",
       "          [-2.7731e-02, -5.6657e-02,  5.1986e-03],\n",
       "          [ 8.6830e-02, -3.7852e-02, -7.2953e-03]],\n",
       "\n",
       "         [[-2.2769e-02,  5.6863e-03, -3.1986e-02],\n",
       "          [-4.7976e-03,  4.7310e-02,  1.7059e-02],\n",
       "          [ 3.1882e-02, -2.4640e-03, -2.6598e-02]],\n",
       "\n",
       "         [[ 1.6279e-02,  1.3925e-02, -2.5644e-02],\n",
       "          [ 2.4989e-02,  7.7165e-03,  1.2399e-02],\n",
       "          [-4.6510e-02, -3.3550e-02, -3.6780e-02]]]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "x = torch.randn(16,3,128,128)\n",
    "out = net(x).sum()\n",
    "#out.backward()\n",
    "#net.cl0.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.lin.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grouped convolution\n",
    "\n",
    "In some applications, it can be necessary to process a multi-channel (e.g., `C` channels) input in a more refined way. \n",
    "\n",
    "Conventionally, a convolution kernel for a `C`-channel input would also have `C` channels, i.e., it operates over all channels of the input.\n",
    "\n",
    "**Example 1**\n",
    "\n",
    "*Say we want to handle each channel of the input separately*. Assume our input is of size `B x 10 x 32 x 32` and we want to have `50` output channels. This means we obviously need `50` kernels in total, however, each of size `(10/10) x 3 x 3`. This means we need ten `groups`.\n",
    "\n",
    "Now, the first `50/10=5` kernels are applied on the first channel, producing `5` output channels, the next `5` kernels are applied to the second channel and so on. As we have `10` input channels, we get a total of `50` output channels.\n",
    "\n",
    "We also see that the number of input channels, as well as the number of output channels needs to be divisible by the number of groups.\n",
    "\n",
    "**Example 2**\n",
    "\n",
    "Let's change the number of groups from our previous example to `2` (instead of `10` as before) and keep everything else the same.\n",
    "\n",
    "We again need `50` kernels, but now of size `(10/2) x 3 x 3`. \n",
    "There will be `25` kernels in **group 1** and `25` kernels in **group 2**. Group 1 processes the first half of the input channels, group 2 the second half. As the outputs are concatenated (along the channel dimension), we obtain `50` output channels.\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "In principle, you can interpret grouped convolution as first splitting the input into #groups (along the channels) and then processing each group by its own convolution layer with an appropriate number of filters and an appropriate filter size.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 3, 3])\n",
      "torch.Size([1, 50, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,10,32,32)\n",
    "m = nn.Conv2d(10,50,3,1,groups=10, bias=False)\n",
    "print(m.weight.data.size())\n",
    "print(m(x).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 5, 3, 3])\n",
      "torch.Size([1, 50, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,10,32,32)\n",
    "m = nn.Conv2d(10,50,3,1,groups=2, bias=False)\n",
    "print(m.weight.data.size())\n",
    "print(m(x).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great resource for understanding convolution is the convolution arithmetic tutorial by Dumoulin & Visin, which can be found [here](https://github.com/vdumoulin/conv_arithmetic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainset_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=32,shuffle=True, num_workers=1)\n",
    "\n",
    "cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testset_loader = torch.utils.data.DataLoader(cifar_testset, batch_size=32, shuffle=False, num_workers=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
