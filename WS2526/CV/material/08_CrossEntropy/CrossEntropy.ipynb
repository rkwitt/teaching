{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision (911.908)\n",
    "\n",
    "## <font color='crimson'>Cross Entropy</font>\n",
    "\n",
    "**Changelog**:\n",
    "\n",
    "- *Sep. 2020*: initial version (using PyTorch v1.6) \n",
    "- *Sep. 2021*: adaptations to PyTorch v1.9\n",
    "- *Jan. 2022*: adaptations to PyTorch v1.10.1\n",
    "- *Dec. 2022*: adaptations to PyTorch v1.13.1\n",
    "\n",
    "---\n",
    "\n",
    "In this lecture, we learn about the **cross-entropy** loss, i.e., the prevalent loss for training multi-class classifiers, implemented as neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "- [Cross-Entropy](#Cross-Entropy-(CE))\n",
    "- [Practical example](#Practical-example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-squared-error (MSE) for classification problems?\n",
    "\n",
    "Say you have a **classification problem** where the class labels are **one-hot** encoded. Assume the correct class is represented by $1$, i.e., one-hot encoded as\n",
    "\n",
    "$$\\mathbf{y} = [1,0,0]^\\top$$\n",
    "\n",
    "Now, say your model would predict a **score vector** and we take the index (starting at $1$) at the maximum score as our prediction. For a score vector\n",
    "\n",
    "$$\\mathbf{y}' = [0,1.1,1]^\\top$$\n",
    "\n",
    "the MSE to $\\mathbf{y}$ would be $\\approx 1$. Similarly, if your model predicted\n",
    "\n",
    "$$\\mathbf{y}'' = [2,-1,-1.1]^\\top$$\n",
    "\n",
    "we would also get the **same** MSE. However, $\\mathbf{y}''$ is actually correct.\n",
    "\n",
    "**MSE makes a Gaussian noise assumption** (which is fine for regression) around the targets; this is most likely not satisfied in the context of classification. Below is code for the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0700)\n",
      "tensor(1.0700)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "u = torch.tensor([1,0,0], dtype=torch.float32) # a 1 at the position of the class index\n",
    "y = torch.tensor([0,1.1,1], dtype=torch.float32)\n",
    "z = torch.tensor([2,-1,-1.1], dtype=torch.float32)\n",
    "\n",
    "print(F.mse_loss(u,y, reduction='mean'))\n",
    "print(F.mse_loss(u,z, reduction='mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy (CE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's motivate the **cross-entropy loss** from the viewpoint maximizing the posterior probability of labels given the data.\n",
    "\n",
    "\n",
    "In the following, assume $y$ identifies the label and $\\mathbf{x} \\in \\mathbb{R}^d$ is some input sample. Also, we assume that tuples of the form $(\\mathbf{x},y)$ are iid and labels ($y$) take values in $\\{1,\\ldots,C\\}$ (so, a $C$-class problem). According to Bayes' rule, we have (for the posterior probability of class $i$)\n",
    "\n",
    "$$p(y=i | \\mathbf{x}) = \\frac{p(\\mathbf{x}|y=i)p(y=i)}{\\sum_c p(\\mathbf{x}|y=c)p(y=c)}$$\n",
    "\n",
    "Upon defining\n",
    "\n",
    "$$ a_i = \\log(p(\\mathbf{x}|y=i)p(y=i))$$\n",
    "\n",
    "we can write\n",
    "\n",
    "$$p(y=i | \\mathbf{x}) = \\frac{e^{a_i}}{\\sum_c e^{a_c}}$$\n",
    "\n",
    "Now, let's make the assumption that the class-conditional probabilities, $p(\\mathbf{x}|y=i)$, are Gaussian with mean $\\boldsymbol{\\mu}_i$ and\n",
    "identity covariance $\\boldsymbol{\\Sigma} = \\mathbf{I}$. \n",
    "Formally,\n",
    "\n",
    "$$p(\\mathbf{x}|y=i) = \\frac{1}{(2\\pi)^{d/2}} e^{-0.5 \\| \\mathbf{x}-\\boldsymbol{\\mu}_i \\|^2}$$\n",
    "\n",
    "Writing out $a_i$ from above gives\n",
    "\n",
    "$$a_i = -0.5\\mathbf{x}^\\top\\mathbf{x} + \\boldsymbol{\\mu}_i^\\top\\mathbf{x} - 0.5 \\boldsymbol{\\mu}_i^\\top\\boldsymbol{\\mu}_i + \\log(p(y=i)) + \\log\\left(\\frac{1}{(2\\pi)^{d/2}} \\right)$$\n",
    "\n",
    "Upon setting $\\mathbf{w}_i = \\boldsymbol{\\mu}_i$, we get \n",
    "\n",
    "$$a_i = -0.5\\mathbf{x}^\\top\\mathbf{x} + \\mathbf{w}_i^\\top\\mathbf{x} \\underbrace{- 0.5 \\mathbf{w}_i^\\top\\mathbf{w}_i + \\log(p(y=i)) + \\log\\left(\\frac{1}{(2\\pi)^{d/2}} \\right)}_{b_i}$$\n",
    "\n",
    "or (with terms aggregated) \n",
    "\n",
    "$$a_i = -0.5\\mathbf{x}^\\top\\mathbf{x} + \\mathbf{w}_i^\\top\\mathbf{x}  + b_i$$\n",
    "\n",
    "where we have subsumed most of the terms into $b_i$. Now, \n",
    "\n",
    "$$p(y=i | \\mathbf{x}) = \\frac{e^{a_i}}{\\sum_c e^{a_c}} = \\frac{e^{-0.5\\mathbf{x}^\\top\\mathbf{x}} e^{\\mathbf{w}_i^\\top\\mathbf{x}}} {\\sum_c e^{-0.5\\mathbf{x}^\\top\\mathbf{x}} e^{\\mathbf{w}_c^\\top\\mathbf{x}}} = \\frac{e^{\\mathbf{w}_i^\\top\\mathbf{x}}} {\\sum_c e^{\\mathbf{w}_c^\\top\\mathbf{x}}}$$\n",
    "\n",
    "which is the output of a linear transformation of the $\\mathbf{x}$ pushed through a so called **softmax** function. In particular, the output of the softmax at the $i$-th coordinate is\n",
    "\n",
    "$$\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_c e^{z_c}}$$\n",
    "\n",
    "If we now have $N$ (iid, independent and identically distributed) samples $(\\mathbf{x}_1,y_1),\\ldots,(\\mathbf{x}_N,y_N)$ with $\\forall i: y_i \\in \\{1,\\ldots,C\\}$, we can follow the goal of **maximizing the likelihood** (or, equivalently, minimizing the **negative log-likelihood**) to obtain the following loss function\n",
    "\n",
    "$$-\\frac{1}{N} \\sum_{i=1}^N \\log(p(y=y_i|\\mathbf{x}_i))$$\n",
    "\n",
    "In particular, let's say we have a model $f_{\\mathbf{W}}$ which linearly maps inputs $\\mathbf{x} \\in \\mathbb{R}^d$ to $\\mathbb{R}^C$. This corresponds to one layer of neurons whose output can be written as\n",
    "\n",
    "$$f_{\\mathbf{W}}(\\mathbf{x}) = \\mathbf{W}\\mathbf{x} = [\\mathbf{w}_1^\\top\\mathbf{x},\\ldots,\\mathbf{w}_C^\\top\\mathbf{x}]^\\top$$\n",
    "\n",
    "where we have aggregated all weights into a matrix $\\mathbf{W}$ which parametrizes this layer of neurons.\n",
    "For a single training instance $(\\mathbf{x},y)$ and assuming that $y=k$, we can write the aforementioned loss function (without the $1/N$ factor and the summation) as\n",
    "\n",
    "$$l(\\mathbf{W},(\\mathbf{x},y)) = -\\log\\left(\n",
    "\\frac{e^{[f_\\mathbf{w}(\\mathbf{x})]_k}}{\\sum_c e^{[f_\\mathbf{w}(\\mathbf{x})]_c} }\\right) = -\\log(\\text{softmax}([ f_{\\mathbf{w}}(\\mathbf{x})]_k)\n",
    "$$\n",
    "\n",
    "Equivalently, we can write\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "l(\\mathbf{W},(\\mathbf{x},y)) & = - \\sum_k \\delta_{y}(k) \\log\\left(\n",
    "\\frac{e^{[f_\\mathbf{w}(\\mathbf{x})]_k}}{\\sum_c e^{[f_\\mathbf{w}(\\mathbf{x})]_c} }\\right) \\\\ \n",
    "& = - \\sum_k \\delta_{y}(k) \\log(p(y=k|\\mathbf{x})) \\\\\n",
    "& = \\mathbb{H}(\\delta_y, p(y=\\cdot|\\mathbf{x})))\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\delta_y(k) = \\begin{cases}\n",
    "1 & \\text{if}~y=k \\\\\n",
    "0 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\mathbb{H}(p,q) = -\\sum_k p(k)\\log(q(k))$$\n",
    "\n",
    "being the **cross-entropy** between distributions $p$ and $q$ (discrete).\n",
    "\n",
    "In other words, we compute the **cross-entropy** between the true posterior $\\delta_y$ and the estimated posterior $p(y=\\cdot|\\mathbf{x})$, noting that the true posterior is a vector of all zeros, except for a 1 at the $k$-th position (as the true label of $\\mathbf{x}$ is $k$).\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "From an implementation point of view, the discussed example basically corresponds to a ``nn.Linear`` layer, followed by a `nn.LogSoftmax` layer and the ``torch.nn.NLLLoss``\n",
    "as a loss function. For convenience, PyTorch implements the same functionality in the ``nn.CrossEntropyLoss`` which directly sits on top of the linear layer (hence, no need for the softmax module). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(5,3) # e.g. for a 3-class problem\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x,dim=1) # log(softmax(.)) -> use NLL (negative log-likelihood)\n",
    "        return x\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(5,3)   # no log(softmax(.)) -> use PyTorch's CE directly\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "x = torch.randn(1,5)\n",
    "y = torch.tensor([1], dtype=torch.long)\n",
    "\n",
    "W = torch.rand(3,5) # manually set the weights\n",
    "b = torch.rand(1,1) # manually set the biases\n",
    "\n",
    "# create two networks\n",
    "net0 = Net0() \n",
    "net1 = Net1()\n",
    "\n",
    "# init weights - we do this here manually, just to make sure\n",
    "# we have the same f_W(.)\n",
    "net0.fc.weight.data = W\n",
    "net0.fc.bias.data = b\n",
    "\n",
    "net1.fc.weight.data = W\n",
    "net1.fc.bias.data = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Net0 (with LogSoftmax)\n",
      "\n",
      "[[-0.6394741 -1.36645   -1.5259264]]\n",
      "\n",
      "Output of Net1 (without LogSoftmax)\n",
      "\n",
      "[[ 0.5333383  -0.19363755 -0.3531139 ]]\n",
      "\n",
      "Loss terms\n",
      "----------\n",
      "using NLLLoss:         1.366\n",
      "using CrossEntropyLoss 1.366\n"
     ]
    }
   ],
   "source": [
    "loss_fn0 = nn.NLLLoss()          # use for Net0\n",
    "loss_fn1 = nn.CrossEntropyLoss() # use for Net1\n",
    "\n",
    "o0 = net0(x)\n",
    "o1 = net1(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Output of Net0 (with LogSoftmax)\\n')\n",
    "print(o0.detach().numpy())\n",
    "print('\\nOutput of Net1 (without LogSoftmax)\\n')\n",
    "print(o1.detach().numpy())\n",
    "\n",
    "print('\\nLoss terms')\n",
    "print('----------')\n",
    "\n",
    "l0 = loss_fn0(o0, y)\n",
    "l1 = loss_fn1(o1, y)\n",
    "print('using NLLLoss:         {:.3f}'.format(l0.item()))\n",
    "print('using CrossEntropyLoss {:.3f}'.format(l1.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T=0.001 # temperature\n",
    "a = torch.tensor([2.1/T,5.5/T,3.3/T])\n",
    "F.softmax(a,dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
