{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision (911.908)\n",
    "\n",
    "## <font color='crimson'>Automatic differentiation (aka AutoGrad)</font>\n",
    "\n",
    "**Changelog**:\n",
    "- *Sep. 2020*: initial version (using PyTorch v1.6) \n",
    "- *Sep. 2021*: adaptations to PyTorch v1.9\n",
    "- *Oct. 2022*: adaptations to PyTorch v1.12.1 + minor fixes\n",
    "\n",
    "---\n",
    "\n",
    "In this lecture, we take a look at the mechanics of **automatic differentiation** (aka *AutoGrad* / *AutoDiff*) in PyTorch. These techniques are at the very heart of today's frameworks for learning with neural networks.\n",
    "\n",
    "You can also find the official (tutorial-like) documentation [here](https://pytorch.org/docs/stable/notes/autograd.html).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introductory example](#Introductory-example)\n",
    "- [A  slightly more involved example](#A-slightly-more-involved-example)\n",
    "- [The AutoGrad machinery](#The-AutoGrad-machinery)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Running this code requires torchviz. Install via\n",
    "#\n",
    "# pip install torchviz\n",
    "#\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import agtree2dot\n",
    "\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from utils import visualize_DAG\n",
    "from IPython.display import HTML\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Introductory example\n",
    "\n",
    "In this example, we take the following function\n",
    "\n",
    "$$f(x;w,b) = \\max\\{wx + b, 0\\} = \\text{ReLU}(wx+b)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\text{ReLU}: x \\mapsto \\text{ReLU}(x) = \\max\\{x, 0\\}\\enspace.$$\n",
    "\n",
    "We first look at its representation as a **computation graph** (with example values for $w,b$ and $x$ added):\n",
    "\n",
    "<img src=\"DAG0.svg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compute  \n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial b} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial b}$$\n",
    "\n",
    "First, \n",
    "\n",
    "$$ \\frac{\\partial v}{\\partial b} = 1 $$\n",
    "\n",
    "and (using a sub-gradient)\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial v} = \\frac{\\partial}{\\partial v} \\text{ReLU}(v) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if}~v \\leq 0\\\\\n",
    "1 & \\text{else}\n",
    "\\end{cases}\\enspace.\n",
    "$$\n",
    "\n",
    "Hence, in our case, we have (as $v=7$)\n",
    "\n",
    "$$\\frac{\\partial a}{\\partial v} = 1$$\n",
    "\n",
    "and consequently \n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial b} = 1\\enspace.\n",
    "$$\n",
    "\n",
    "Let's try to compute the partial derivative of $a$ wrt. $w$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial u}\\frac{\\partial u}{\\partial w}\n",
    "$$\n",
    "\n",
    "We have, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial v}{\\partial u} = 1\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial w} = x\n",
    "$$\n",
    "\n",
    "Combined, we obtain (if we use the fact that $x=3$)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial u}\\frac{\\partial u}{\\partial w} = 1\\cdot 1\\cdot 3 = 3\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Conceptually, the **forward pass** (i.e., the computation of the function value) is a sequence of standard tensor computations (i.e., in this example, simply the computation of all intermediate results). We need the **Directed Ascyclic Graph (DAG)** of tensor operations only to compute derivatives (via the chain rule). \n",
    "\n",
    "When executing tensor operations, PyTorch can automatically (on-the-fly) construct the graph of operations to compute the gradient of any quantity with respect to any tensor involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F # functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Let's implement the simple example from above. Note the use of `requires_grad=True` (`False` by default) for the variable `w` and `b` (explained below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.], grad_fn=<ClampBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.])\n",
    "w = torch.tensor([2.], requires_grad=True) \n",
    "b = torch.tensor([1.], requires_grad=True) \n",
    "\n",
    "u = x*w\n",
    "v = u+b\n",
    "a = torch.clamp(v, min=0) # alternatively: a = F.relu(x*w + b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensor has a Boolean field `requires_grad`, set to `False` by default, which\n",
    "states if PyTorch should build the graph of operations so that gradients with\n",
    "respect to it can be computed.\n",
    "\n",
    "**Note**: Only *floating point type* tensors can have their gradient computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = 7.0\n"
     ]
    }
   ],
   "source": [
    "print(\"v =\", v.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient $\\partial a/\\partial w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "grad_w = grad(a, w, retain_graph=True)\n",
    "print(grad_w[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient $\\partial a/\\partial b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(grad(a, b)[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of `retain_graph=True` earlier. This is not required here, but in case you want to execute that cell multiple time, you will need it. The reason is that, by default, the part of the graph that computes `a` is destroyed (for memory reasons) once the gradient computation is done. If `retain_graph=False`, then execution of that statement will fail (unless you do another forward pass). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A slightly more involved example\n",
    "\n",
    "In this example, we have a slightly more complex computation graph with a *shared* weight $w$ and two paths to arrive at the result. The function is\n",
    "\n",
    "$$f(x;w) = (wx)^2 + (wx)^3$$\n",
    "\n",
    "<img src=\"DAG1.svg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the partial derivative of $a$ wrt. $w$ by hand once more:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v_1}\\frac{\\partial v_1}{\\partial u_1}\\frac{\\partial u_1}{\\partial w} + \\frac{\\partial a}{\\partial v_2}\\frac{\\partial v_2}{\\partial u_2}\\frac{\\partial u_2}{\\partial w} = \n",
    "1\\cdot 2u_1\\cdot x + 1 \\cdot 3u_2^2 \\cdot x = 8 + 24 = 32\n",
    "$$\n",
    "\n",
    "Again, having **all the intermediate values** during the **forward pass** makes it very easy to actually compute the gradient of $a$ wrt. $w$, i.e., $\\partial a/\\partial w$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 12.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.])\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "u1 = x*w\n",
    "u2 = x*w\n",
    "\n",
    "v1 = torch.pow(u1, 2)\n",
    "v2 = torch.pow(u2, 3)\n",
    "\n",
    "a = v1 + v2\n",
    "print(\"a =\", a.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the gradient of $a$ wrt. $w$, i.e., $\\partial a/\\partial w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0\n"
     ]
    }
   ],
   "source": [
    "print(grad(a, w)[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The AutoGrad machinery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autograd DAG is encoded through the fields `grad_fn` of Tensors, and the\n",
    "fields `next_functions` of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "# below is another way of saying requires_grad=True\n",
    "x = torch.tensor([ 1.0, -2.0, 3.0, -4.0 ]).requires_grad_() \n",
    "a = x.pow(2.)\n",
    "s = a.sum()\n",
    "print(s.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we visualize a simple computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 2.]).requires_grad_()\n",
    "q = x.norm() # L2-norm of tensor sqrt(1^2+2^2+2^2)=3\n",
    "print(q.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"simple1.jpg\" align=\"center\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agtree2dot.save_dot(q, {x: 'x', q: 'q'}, open('simple1.dot', 'w'))\n",
    "HTML(visualize_DAG('simple1.dot', 'simple1.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Here's another, slightly more involved, example: We have $\\mathbf{x} \\in \\mathbb{R}^{10}$ and **first** compute (noting that $\\text{tanh}$ is applied componentwise).\n",
    "\n",
    "$$\n",
    "\\mathbf{h}  = \\text{tanh}(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) \\\\\n",
    "$$\n",
    "\n",
    "with $\\mathbf{W}_1 \\in \\mathbb{R}^{20 \\times 10}$ and $\\mathbf{b}_1 \\in \\mathbb{R}^{20}$. This results in $\\mathbf{h} \\in \\mathbb{R}^{20}$. **Second**, we compute\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}}  = \\text{tanh}(\\mathbf{W}_2\\mathbf{h} + \\mathbf{b}_2)\n",
    "$$\n",
    "\n",
    "with $\\mathbf{W}_2 \\in \\mathbb{R}^{5 \\times 20}$ and $\\mathbf{b}_2 \\in \\mathbb{R}^5$. This results in \n",
    "$\\hat{\\mathbf{y}} \\in \\mathbb{R}^5$. And **finally**, we compute (the mean-squared error)\n",
    "\n",
    "$$\n",
    "L(\\mathbf{y},\\hat{\\mathbf{y}}) = \\frac{1}{5} \\sum_{i=1}^5(y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $y_i,\\hat{y}_i$ are the $i$-th elements of $\\mathbf{y}$ and $\\hat{\\mathbf{y}}$, respectively. In the code below, $y_i$ (which we use as targets) are just random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2811025083065033\n"
     ]
    }
   ],
   "source": [
    "w1 = torch.rand(20, 10).requires_grad_()\n",
    "b1 = torch.rand(20).requires_grad_()\n",
    "w2 = torch.rand(5, 20).requires_grad_()\n",
    "b2 = torch.rand(5).requires_grad_()\n",
    "\n",
    "x = torch.rand(10)\n",
    "h = torch.tanh(w1 @ x + b1)\n",
    "yhat = torch.tanh(w2 @ h + b2)\n",
    "\n",
    "y = torch.rand(5)\n",
    "\n",
    "loss = (y - yhat).pow(2).mean()\n",
    "loss.backward() # computes the partial derivatives wrt. w1,b1,w2,b2\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the computation graph ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"simple2.jpg\" align=\"center\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agtree2dot.save_dot(loss, \n",
    "                    {\n",
    "                        w1: 'w1',\n",
    "                        b1: 'b1',\n",
    "                        w2: 'w2',\n",
    "                        b2: 'b2',\n",
    "                        loss: 'loss'\n",
    "                    }, open('simple2.dot', 'w'))\n",
    "HTML(visualize_DAG('simple2.dot', 'simple2.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `torch.no_grad()`\n",
    "\n",
    "The `torch.no_grad()` **context** switches off the autograd machinery, and can be\n",
    "used for operations such as parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333333 -0.333333\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor( 0.7).requires_grad_()\n",
    "b = torch.tensor(-0.7).requires_grad_()\n",
    "eta = 0.1\n",
    "\n",
    "for k in range(100):\n",
    "    l = (a - 1)**2 + (b + 1)**2 + (a - b)**2 # i.e., the forward pass\n",
    "    ga, gb = torch.autograd.grad(l, (a, b))\n",
    "\n",
    "    # walk towards the direction of the negative gradient \n",
    "    # (aka gradient descend) - both statements will not change\n",
    "    # the computation graph, because they are within the \n",
    "    # torhc.no_grad context!\n",
    "    with torch.no_grad():\n",
    "        a -= eta * ga \n",
    "        b -= eta * gb\n",
    "        \n",
    "print('%.06f' % a.item(), '%.06f' % b.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you do not believe that this is the minimum, fire up Mathematica and run the `2DMinimumExample.nb` available in this directory :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `detach()`\n",
    "\n",
    "The `detach()` method creates a tensor which shares the data, but does not\n",
    "require gradient computation, and is not connected to the current computation graph.\n",
    "\n",
    "This method should be used when the gradient should not be propagated\n",
    "beyond a variable, or to update leaf tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n",
      "tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(10, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y = x**2\n",
    "z = x**3\n",
    "r=(y+z).sum()\n",
    "print(r.item())\n",
    "r.backward() # computes the gradient of r wrt. x\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets exclude the contribution of the $x^3$ branch via `detach()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(10, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y = x**2\n",
    "z = (x**3).detach()\n",
    "r=(y+z).sum()\n",
    "print(r.item())\n",
    "r.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting back to our exampe of finding the minimum of a function (see above) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000 -0.000000\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor( 0.5).requires_grad_()\n",
    "b = torch.tensor(-0.5).requires_grad_()\n",
    "eta = 0.1\n",
    "\n",
    "for k in range(100):\n",
    "    l = (a - 1)**2 + (b + 1)**2 + (a.detach() - b)**2\n",
    "    ga, gb = torch.autograd.grad(l, (a, b))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= eta * ga\n",
    "        b -= eta * gb\n",
    "        \n",
    "print('%.06f' % a.item(), '%.06f' % b.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
